{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-981-32-9294-9_28/MediaObjects/483279_1_En_28_Fig1_HTML.png\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2024\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-981-32-9294-9_28/MediaObjects/483279_1_En_28_Fig1_HTML.png</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: Regresión Logística</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión logística es un algoritmo de machine learning para clasificación que es usado para predecir la probabilidad de variables dependientes categóricas. \n",
    "\n",
    "En la regresión logística la variable dependiente es una variable binaria que contiene como 1 (sí, ganar, etc) o 0 (no, perder, etc). \n",
    "\n",
    "La regresión logística es también conocida como la \"Regresión logística binomial\", la cual es basada en la función sigmoidal donde la salida es la probabilidad y la entrada puede ir desde -infinito a +infinito. \n",
    "\n",
    "**Supuestos de regresión logística**\n",
    "\n",
    "- La regresión logística binaria requiere que la variable dependiente sea binaria.\n",
    "- Para una regresión binaria, el nivel de factor 1 de la variable dependiente debe representar el resultado deseado.\n",
    "- Solo deben incluirse las variables significativas.\n",
    "- Las variables independientes deben ser independientes entre sí. Es decir, el modelo debe tener poca o ninguna multicolinealidad.\n",
    "- Las variables independientes están relacionadas linealmente con las probabilidades logarítmicas.\n",
    "- La regresión logística requiere tamaños de muestra bastante grandes.\n",
    "\n",
    "**¿Puede ser utilizada en problemas multiclase?**\n",
    "\n",
    "Sí... utiliza el método de One vs Rest (One vs all)\n",
    "Donde hace problemas binarios para cada combinación de clases y predice la clase con la probabilidad más alta. \n",
    "\n",
    "\n",
    "### En otro tema... lidiar con clases imbalanceadas en el target\n",
    "\n",
    "**¿Cómo saber cuándo hay que balancear las clases?**\n",
    "\n",
    "1. Imbalanceo severo. Cuando una clase es significativamente más frecuente (Ej. 90% vs 10% o peor). \n",
    "2. Importancia del problema a resolver. Si la clase minoritaria representa un resultado crítico (ej. detectar fraudes, diagnósticos médicos, fallas, etc.), balancear clases se vuelve necesario para evitar fallar en predicciones importantes. \n",
    "\n",
    "**¿Cuándo evitar el balanceo de clases?**\n",
    "1. Balanceo no tan severo. Si el imbalanceo es menor (ej. 60% vs 40%), balancear puede que no sea tan necesario y hasta puede ser perjudicial ya que puede llevar al overfitting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Se tienen datos de campañas de marketing (llamadas telefónicas) de un banco portugués. Se tiene la necesidad de predecir si un cliente va a suscribirse a un depósito a término (variable a predecir). \n",
    "\n",
    "Un depósito a término es un depósito que un banco ofrece con una tasa fija en la cual el dinero se regresará en cierto tiempo de madurez. \n",
    "\n",
    "\n",
    "### Los datos\n",
    "\n",
    "Los datos se obtuvieron del repositorio de UCI Machine learning https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "Consiste de 41188 datos. \n",
    "\n",
    "\n",
    "Variables de entrada:\n",
    "\n",
    "- age (numerica)\n",
    "- job : tipo de trabajo (categorica: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "- marital : estado marital (categorica: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "- education (categorica: basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "- housing: tiene hipoteca? (categorica: 'no','yes','unknown')\n",
    "- loan: tiene préstamos personales? (categorica: 'no','yes','unknown')\n",
    "- contact: tipo de comunicación (categorical: 'cellular','telephone')\n",
    "- month:último mes de contacto del año (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "- day_of_week: último día de contacto de la semana (categorical: 'mon','tue','wed','thu','fri')\n",
    "- duration: duración en segundos de la llamada. \n",
    "- campaign: número de llamadas realizadas durante esta campaña y para este cliente (numeric, includes last contact)\n",
    "- pdays: número de días que pasaron después de que el cliente fue contactado de la campaña anterior (numeric; 999 means client was not previously contacted)\n",
    "- previous: número de contactos realizados antes de esta campaña y para este cliente (numeric)\n",
    "- poutcome: resultado de la campaña de marketing anterior (categorical: 'failure','nonexistent','success')\n",
    "- emp.var.rate: tasa de variación del empleo - indicador trimestral (numeric)\n",
    "- cons.price.idx: índice de precios al consumidor - indicador mensual  (numeric)\n",
    "- cons.conf.idx: índice de confianza del consumidor - indicador mensual (numeric)\n",
    "- euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "\n",
    "\n",
    "Variable de salida:\n",
    "- y - se suscribió el cliente a un depósito a término? (binario: 'yes','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar datos\n",
    "data = pd.read_csv('bank_full.csv')\n",
    "#Quitar valores nulos\n",
    "data = data.dropna()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cómo se ve la distribución de nuestra variable de salida\n",
    "data['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4640/(4640+36548)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficando la distribucion de la variable a predecir\n",
    "sns.countplot(x='y', data=data)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de salida están imbalanceados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver la distribución de las variables contra la variable de salida \"Y\" para empezar a ver qué variables podemos quitar o dejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizar variable Y vs tipo de trabajo\n",
    "pd.crosstab(data.job, data.y).plot(kind='bar')\n",
    "plt.title('Tipo de trabajo vs compra')\n",
    "plt.xlabel('Tipo de trabajo')\n",
    "plt.ylabel('Proporcion de clientes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizar variable Y vs estatus marital\n",
    "table=pd.crosstab(data.marital,data.y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Estado marital vs compra')\n",
    "plt.xlabel('Estado marital')\n",
    "plt.ylabel('Proporcion de clientes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El estado marital no parece ser un predictor bueno para predecir la compra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizar variable Y vs educación\n",
    "table=pd.crosstab(data.education,data.y)\n",
    "table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\n",
    "plt.title('Educacion vs compra')\n",
    "plt.xlabel('Educacion')\n",
    "plt.ylabel('Proporcion de clientes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La educación parece ser un buen predictor para la variable a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizar variable Y vs día de la semana\n",
    "pd.crosstab(data.day_of_week,data.y).plot(kind='bar')\n",
    "plt.title('Día de la semana vs compra')\n",
    "plt.xlabel('Día')\n",
    "plt.ylabel('Proporcion de clientes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El día de la semana puede no ser muy buen predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizar variable Y vs mes\n",
    "pd.crosstab(data.month,data.y).plot(kind='bar')\n",
    "plt.title('Mes vs compra')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Proporcion de clientes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mes puede ser un buen predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribución de las edades\n",
    "data.age.hist()\n",
    "plt.title('Histograma de edad')\n",
    "plt.xlabel('Edad')\n",
    "plt.ylabel('Frecuencia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de los clientes del banco están entre los 30-40 años"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir X de Y\n",
    "X = data.loc[:,data.columns!='y']\n",
    "y = data.loc[:,data.columns=='y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir en test y train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='y', data=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpiar variables categóricas**\n",
    "\n",
    "Vamos a usar one-hot encoding para convertir variables categóricas a numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos las variables numericas de las categoricas\n",
    "numeric_features = X_train.select_dtypes(include=['float64', 'int64']).columns.values\n",
    "numeric_features = numeric_features[numeric_features != 'y']\n",
    "\n",
    "category_features = X_train.select_dtypes(include=['object', 'bool']).columns.values\n",
    "\n",
    "print(\"Variables numericas:\", numeric_features)\n",
    "print(\"Variables categoricas:\",category_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crean dos pipelines: uno para transformar las variables numéricas y otro para las categóricas.\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Se combina ambos transformadores (numérico y categórico) en un solo preprocesador que puede aplicarse a los datos para procesar todas las variables en conjunto.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, category_features)])\n",
    "\n",
    "ohe = preprocessor.fit(X_train)\n",
    "\n",
    "X_train_t = ohe.transform(X_train)\n",
    "X_test_t = ohe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aunque OneHotEncoder ya realiza la codificación de variables categóricas, la función dummify tiene dos propósitos:\n",
    "#1. Obtener los Nombres de las Columnas Generadas\n",
    "# 2. Convertir el Array Transformado en un DataFrame\n",
    "\n",
    "def dummify(ohe, x, columns):\n",
    "    transformed_array = ohe.transform(x)\n",
    "\n",
    "    enc = ohe.named_transformers_['cat'].named_steps['onehot']\n",
    "    feature_lst = enc.get_feature_names_out(category_features.tolist())   \n",
    "    \n",
    "    cat_colnames = np.concatenate([feature_lst]).tolist()\n",
    "    all_colnames = numeric_features.tolist() + cat_colnames \n",
    "    \n",
    "    df = pd.DataFrame(transformed_array, index = x.index, columns = all_colnames)\n",
    "    \n",
    "    return transformed_array, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t_array, X_train_t = dummify(ohe, X_train, category_features)\n",
    "X_test_t_array, X_test_t = dummify(ohe, X_test, category_features)\n",
    "\n",
    "X_train_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sobremuestreo Synthetic Minority Oversampling Technique (SMOTE)\n",
    "from IPython.display import Image\n",
    "Image(filename='SMOTE.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordar que el balanceo de clases es después de dividir los datos en train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(random_state=0)\n",
    "\n",
    "#Hacer oversampling en datos del train\n",
    "os_data_X, os_data_y=os.fit_resample(X_train_t, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X,columns=X_train_t.columns )\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto ya los datos están balanceados. Si se fijan sólo hicimos el oversampling en los datos de entrenamiento, ninguna de la información de los datos de test fueron usados para crear muestras sintéticas, por lo tanto ninguna información del test se filtra al entrenamiento del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='y', data=os_data_y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de variables\n",
    "\n",
    "Usamos el algoritmo de Recursive Feature Elimination (RFE) para seleccionar variables considerando cada vez menos y menos conjuntos de variables. \n",
    "\n",
    "RFE es fácil de configurar y bastante eficaz a la hora de seleccionar funciones en un conjunto de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#crear modelo de regresión logística\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=2000)\n",
    "\n",
    "#crear el recursive feature elimination para la regresión logística\n",
    "rfe = RFE(model, n_features_to_select=20, verbose=0) #vamos a dejar sólo 20 variables\n",
    "rfe = rfe.fit(os_data_X, os_data_y.values.ravel())\n",
    "print(\"Características seleccionadas: %s\" % rfe.support_)\n",
    "print(\"Rank de las características: %s\" % rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_columns = X_train_t.columns\n",
    "selected_columns = X_train_columns[rfe.support_]\n",
    "print(selected_columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = os_data_X[selected_columns.tolist()]\n",
    "y_train_final = os_data_y['y']\n",
    "X_test_final = X_test_t[selected_columns.tolist()]\n",
    "y_test_final = y_test\n",
    "\n",
    "X_test_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install stastmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementar modelo\n",
    "\n",
    "# statsmodels es un paquete que proporciona funciones para la estimación de muchos modelos estadísticos,\n",
    "#así como para realizar pruebas estadísticas y exploración de datos estadísticos.\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y_train_final,X_train_final)\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p value > 0.05, significa que podemos quitar la variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los p-values para la mayoría de las variables son menores a 0.05, excepto por 1 variable, por lo tanto la vamos a quitar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['duration', 'emp_var_rate', 'cons_price_idx', 'euribor3m',\n",
    "       'job_retired', 'job_unknown', 'marital_unknown',\n",
    "       'education_illiterate', 'default_no', 'contact_telephone', 'month_aug',\n",
    "       'month_jun', 'month_mar', 'month_may', 'month_nov', 'month_oct',\n",
    "       'poutcome_failure', 'poutcome_success']\n",
    "logit_model=sm.Logit(y_train_final,X_train_final[cols])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que terminamos de seleccionar variables, creamos el modelo de regresión logística. \n",
    "\n",
    "Pero antes... ¿Cómo interpretar estos coeficientes?\n",
    "\n",
    "- Para las variables numéricas que se les aplicó escalamiento: los coeficientes representan el efecto de un aumento de una desviación estándar en la variable sobre las probabilidades logarítmicas de la variable objetivo.\n",
    " \n",
    "Ejemplo: Para la variable de \"duration\". Un coeficiente de 1.83 significa que si la duración de la llamada aumenta en una desviación estándar por encima del promedio, las probabilidades de que el cliente acepte la oferta aumentan por un factor de 6.23.\n",
    "\n",
    "- Para las variables categóricas que se les aplicó el one-hot encoding: Los coeficientes de estas características binarias representan el cambio en las probabilidades logarítmicas de la variable objetivo al pasar de la categoría de referencia a la categoría representada por la característica.\n",
    "\n",
    "Ejemplo: Para la variable categórica \"month_nov\". El coeficiente negativo (-1.4411) indica que estar en noviembre disminuye significativamente las probabilidades de que los clientes acepten la oferta.\n",
    "Específicamente, estar en noviembre reduce las odds de aceptar la oferta a aproximadamente un 23.6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regresión logítica con sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#Inicializar objeto\n",
    "logreg = LogisticRegression()\n",
    "#Ajustar modelo a datos de entrenamiento\n",
    "logreg.fit(X_train_final[cols], y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecir con datos del test\n",
    "y_pred = logreg.predict(X_test_t[cols])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para clasificación usamos otras métricas diferentes a las de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado nos dice que tenemos 9390+1226=10616 predicciones correctas y 1575+166=1741 predicciones incorrectas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcular Accuracy\n",
    "print('Accuracy de la regresión logística en los datos de test: {:.4f}'.format(logreg.score(X_test_t[cols], y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(9390+1226)/(9390+1226+166+1575)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas de la regresión logística**\n",
    "\n",
    "- Fácil de implementar, interpretar y muy eficiente de entrenar\n",
    "- No sólo provee la medida de la importancia (tamaño) del coeficiente, sino que también nos dice la dirección de la asociación (positiva/negativa).\n",
    "- Es muy rápida clasificando datos nuevos\n",
    "- Tiene buena precisión para datos simples y funciona bien cuando los datos son linealmente separables\n",
    "- Se pueden interpretar los coeficientes del modelo como indicadores de importancia de variables\n",
    "- La regresión logística hace poco sobre-ajuste cuando los datos son simples pero puede sobre-ajustar cuando tenemos datos de alta dimensionalidad. \n",
    "\n",
    "**Desventajas de la regresión logística**\n",
    "- Si el número de observaciones (filas) es menor que el número de variables (columnas) la regresión logística no se debe usar, sino lo que puede pasar es que sobre ajuste\n",
    "- La mayor limitación de la regresión logística es que asume una relación lineal entre variables dependientes y variables independientes. \n",
    "- No es un buen modelo si no tenemos datos linealmente separables. \n",
    "- Es difícil obtener resultados cuando tenemos relaciones de datos complejos, las redes neuronales pueden mejorar mucho este algoritmo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
