{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://miro.medium.com/v2/resize:fit:592/1*i0o8mjFfCn-uD79-F1Cqkw.png\" width=\"350px\" height=\"180px\" />\n",
    "\n",
    "\n",
    "# <font color= #8A0829> Laboratorio de Modelado de Datos </font>\n",
    "#### <font color= #2E9AFE> `Martes y Viernes (Videoconferencia) de 13:00 - 15:00 hrs`</font>\n",
    "- <Strong> Sara Eugenia Rodríguez </Strong>\n",
    "- <Strong> Año </Strong>: 2024\n",
    "- <Strong> Email: </Strong>  <font color=\"blue\"> `cd682324@iteso.mx` </font>\n",
    "___\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://miro.medium.com/v2/resize:fit:592/1*i0o8mjFfCn-uD79-F1Cqkw.png</p>\n",
    "\n",
    "### <font color= #2E9AFE> Tema: Modelos basados en Árboles Parte II - Clasificación</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bosques Aleatorios\n",
    "\n",
    "\"Los árboles tienen un sólo aspecto que previene que sean la herramienta ideal para el aprendizaje predictivo, que es la **inexactitud**\" \n",
    "\n",
    "Pasos para crear un bosque aleatorio:\n",
    "- Crear un dataset \"bootstrapped\" con reemplazo\n",
    "- Crear un árbol de decisión usando el dataset \"bootstrapped\", pero sólo usar un subconjunto aleatorio de variables (o columnas) en cada paso. \n",
    "- Regresar al paso 1. y repetir \n",
    "\n",
    "Gracias al proceso de bootstrapping, el requerimento de dividir los datos en prueba y entrenamiento no es tan estricto. Se recomienda dividir los datos en prueba y entrenamiento cuando se quiere comparar su desempeño contra otros modelos. \n",
    "\n",
    "\n",
    "**Hiperparámetros:**\n",
    "- max_depth: Puedo limitar hasta qué profundidad quiero que crezca cada árbol en mi bosque aleatorio.\n",
    "- min_sample_split: le indica al árbol el número mínimo de observaciones requeridas en cualquier nodo dado para dividirlo.\n",
    "- min_samples_leaf: le indica al árbol el número mínimo de observaciones requeridas en la hoja final.\n",
    "- n_estimators: numero de árboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import (accuracy_score,precision_score,recall_score)\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar la base de datos\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1], c=Y, s=10, cmap=plt.cm.rainbow,zorder=2)\n",
    "plt.xlabel('x_p1')\n",
    "plt.ylabel('x_p2')\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribucion de la variable de salida\n",
    "unique_values = np.unique(Y)\n",
    "counts = np.bincount(Y)\n",
    " \n",
    "print(\"Valores:\", unique_values)\n",
    "print(\"Cuenta:\", counts[unique_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#dividimos nuestros datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=True, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Construccion y entrenamiento de la bolsa de modelos\n",
    "modelo = RandomForestClassifier(n_estimators=100, #número de árboles en el bosque\n",
    "                               criterion='gini', \n",
    "                               max_depth=10, #profundidad\n",
    "                               min_samples_split=2,\n",
    "                               min_samples_leaf=2,\n",
    "                               bootstrap=True,\n",
    "                               oob_score=False,\n",
    "                               random_state=0,\n",
    "                               verbose=0)\n",
    "\n",
    "start_time = time.time()\n",
    "modelo = modelo.fit(X_train,y_train) # entrenamiento con la bolsa de modelos\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "Yhat = modelo.predict(X_test) #predicción con la bolsa de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion del modelo\n",
    "accu = accuracy_score(y_test,Yhat)\n",
    "prec = precision_score(y_test,Yhat,average='weighted')#añadir weighted cuando es problema multiclase\n",
    "reca = recall_score(y_test,Yhat,average='weighted')\n",
    "print('Accuracy\\t Precision\\t Recall\\n %0.3f\\t %0.3f\\t %0.3f'%(accu,prec,reca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridsearchCV para tunear hiperarámetros\n",
    "\n",
    "Utilicemos cross validation para optimizar hiperparámetros. \n",
    "\n",
    "Antes de hacer el GridsearchCV vamos viendo los hiperparámetros cómo se desarrollan contra las métricas de performance para elegir la mejor malla de valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos la profundidad vs el accuracy\n",
    "max_depths = range(1, 10)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    tree_clas = RandomForestClassifier(max_depth=max_depth, random_state=42)\n",
    "    tree_clas.fit(X_train, y_train)\n",
    "    train_scores.append(tree_clas.score(X_train, y_train))\n",
    "    test_scores.append(tree_clas.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(max_depths, train_scores, label='Train Accuracy', color='blue', marker='o')\n",
    "plt.plot(max_depths, test_scores, label='Test Accuracy', color='green', marker='o')\n",
    "plt.xlabel('Tree Max Depth')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Efecto de la profundidad del bosque en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos probar la malla con 2 profundidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos el min_sample_split vs el accuracy\n",
    "min_samples_splits = range(1, 30)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for min_samples_split in min_samples_splits:\n",
    "    tree_clas = RandomForestClassifier(min_samples_split=min_samples_split, random_state=42)\n",
    "    tree_clas.fit(X_train, y_train)\n",
    "    train_scores.append(tree_clas.score(X_train, y_train))\n",
    "    test_scores.append(tree_clas.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(min_samples_splits, train_scores, label='Train Accuracy', color='blue', marker='o')\n",
    "plt.plot(min_samples_splits, test_scores, label='Test Accuracy', color='green', marker='o')\n",
    "plt.xlabel('Min Sample Split')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Efecto del min. numero de observaciones por split en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos probar la malla entre 27 y 30 min num observaciones por split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graficamos el n_estimators vs el accuracy\n",
    "n_estimators = range(1, 10)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_estimator in n_estimators:\n",
    "    tree_clas = RandomForestClassifier(n_estimators=n_estimator, random_state=42)\n",
    "    tree_clas.fit(X_train, y_train)\n",
    "    train_scores.append(tree_clas.score(X_train, y_train))\n",
    "    test_scores.append(tree_clas.score(X_test, y_test))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(n_estimators, train_scores, label='Train Accuracy', color='blue', marker='o')\n",
    "plt.plot(n_estimators, test_scores, label='Test Accuracy', color='green', marker='o')\n",
    "plt.xlabel('Num Estimators')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Efecto del min. numero del numero de árboles en el Training y Test Performance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podríamos probar la malla con 1 y 2 árboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usando cross validation y grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = RandomForestClassifier(criterion='gini',\n",
    "                               min_samples_leaf=2,\n",
    "                               bootstrap=True,\n",
    "                               oob_score=False,\n",
    "                               random_state=0,\n",
    "                               verbose=0)\n",
    "\n",
    "gs = GridSearchCV(model,\n",
    "                  param_grid = {'max_depth': [2], #profundidad máxima\n",
    "                                'min_samples_split': range(27, 30, 1), #minimo numero de observaciones por split\n",
    "                                'n_estimators': range(1,2,1)}, # número de árboles en el bosque\n",
    "                  cv=2,\n",
    "                  scoring='accuracy')\n",
    "#Entrenamiento\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprimir parámetros óptimos\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear modelo usando parámetros óptimos\n",
    "new_model = RandomForestClassifier(n_estimators=1,\n",
    "                               criterion='gini',\n",
    "                               max_depth=2,\n",
    "                               min_samples_split=27,\n",
    "                               min_samples_leaf=2,\n",
    "                               bootstrap=True,\n",
    "                               oob_score=False,\n",
    "                               random_state=0,\n",
    "                               verbose=0)\n",
    "#Entrenamiento\n",
    "new_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion del modelo\n",
    "yhat = new_model.predict(X_test)\n",
    "accu = accuracy_score(y_test,yhat)\n",
    "prec = precision_score(y_test,yhat,average='weighted')\n",
    "reca = recall_score(y_test,yhat,average='weighted')\n",
    "print('Accuracy\\t Precision\\t Recall\\n %0.3f\\t %0.3f\\t %0.3f'%(accu,prec,reca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importancia de las variables\n",
    "import pandas as pd\n",
    "feature_imp = pd.Series(new_model.feature_importances_, index=iris.feature_names).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=feature_imp, y = feature_imp.index)\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Visualizing Important Features')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ventajas**\n",
    "\n",
    "- Son muy buenos generalizando\n",
    "- Protejen en contra del sobreajuste (overfitting) gracias a la construcción del bootstrapping \n",
    "- También reducen la varianza y por lo tanto mejoran la precisión del modelo\n",
    "- Funcionan muy bien con variables categóricas y variables continuas\n",
    "- No se requiere escalamiento previo de variables \n",
    "- Manejan muy bien el hecho de que haya datos nulos\n",
    "- Son modelos robustos ante valores atípicos (outliers)\n",
    "- Son algoritmos muy estables, cuando hay datos nuevos, el algoritmo no se ve muy afectado. Ya que este nuevo dato puede afectar a un árbol individual, pero es difícil que impacte a todos los árboles. \n",
    "\n",
    "**Desventajas**\n",
    "\n",
    "- Complejidad. Los bosques aleatorios crean muchos árboles y combina sus resultados. Requiere mucho poder computacional y recursos \n",
    "- Periodos de entrenamiento largos. Requieren más tiempo de entrenamiento. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
